\section{Linear Codes}

In this chapter, we introduce structure to our codes by requiring them to be linear subspaces. This allows for more compact representations and often leads to more efficient encoding and decoding algorithms.

\subsection{Finite Fields and Linear Algebra Basics}

To define linear codes, we first need an alphabet that supports arithmetic operations.

\begin{description}
    \item[Finite Field] A \textbf{finite field}, denoted $\mathbb{F}_q$, is a 
        finite set of $q$ elements that comes with addition, subtraction, multiplication, 
        and division operations satisfying standard arithmetic rules (associativity, commutativity, distributivity, identities, and inverses).
    \begin{itemize}
        \item A common example is the field $\mathbb{F}_p$ of integers modulo a prime number $p$.
        \item The size $q$ of any finite field must be a prime power, i.e., $q = p^r$ for some prime $p$ and integer $r \ge 1$.
    \end{itemize}

    \item[Vector Space over $\mathbb{F}_q$] The set of all $n$-tuples of elements from $\mathbb{F}_q$ is denoted $\mathbb{F}_q^n$. This forms a vector space where vector addition and scalar multiplication are performed component-wise over $\mathbb{F}_q$.

    \item[Linear Subspace] A subset $C \subseteq \mathbb{F}_q^n$ is a \textbf{linear subspace} if it is closed under addition and scalar multiplication. That is:
    \begin{itemize}
        \item For any $\vec{c}_1, \vec{c}_2 \in C$, their sum $\vec{c}_1 + \vec{c}_2$ is also in $C$.
        \item For any $\vec{c} \in C$ and any scalar $\alpha \in \mathbb{F}_q$, the product $\alpha\vec{c}$ is also in $C$.
        \item A consequence is that the all-zero vector $\vec{0}$ is always in any linear subspace.
    \end{itemize}
\end{description}

\subsection{Definition and Properties of Linear Codes}

\begin{description}
    \item[Linear Code] A \textbf{linear code} $C$ is a linear subspace of $\mathbb{F}_q^n$.
    \begin{itemize}
        \item If the subspace has dimension $k$, we say it is an $[n, k]$ code.
        \item If we also know its minimum distance is $d$, we call it an $[n, k, d]$ code. The square brackets distinguish linear codes from general $(n, |C|, d)$ codes.
    \end{itemize}

    \item[Generator Matrix ($G$)] An $[n, k]$ linear code $C$ can be described as the row span of a $k \times n$ matrix $G$ whose rows are linearly independent. This is called a \textbf{generator matrix}.
    \[
        C = \{ \vec{m}G \mid \vec{m} \in \mathbb{F}_q^k \}
    \]
    Any message $\vec{m}$ of length $k$ can be encoded into a codeword $\vec{c}$ of length $n$ by the matrix multiplication $\vec{c} = \vec{m}G$.

    \item[Parity-Check Matrix ($H$)] An $[n, k]$ linear code $C$ can also be described as the null space of an $(n-k) \times n$ matrix $H$ of full rank. This is called a \textbf{parity-check matrix}.
    \[
        C = \{ \vec{c} \in \mathbb{F}_q^n \mid H\vec{c}^T = \vec{0} \}
    \]
    This means a vector $\vec{c}$ is a valid codeword if and only if it satisfies the $n-k$ linear constraints (parity checks) defined by the rows of $H$. For any generator matrix $G$ and parity-check matrix $H$ for the same code, it holds that $GH^T = \mathbf{0}$.
\end{description}

\subsection{Distance of Linear Codes}

The linearity of the codes simplifies the calculation of their distance.

\begin{description}
    \item[Hamming Weight] The \textbf{Hamming weight} of a vector $\vec{c}$, denoted $wt(\vec{c})$, is the number of non-zero components in $\vec{c}$. Note that $wt(\vec{c}) = \triangle(\vec{c}, \vec{0})$.
\end{description}

\begin{theorem}
    The minimum distance $d$ of a non-trivial linear code $C$ is equal to the minimum Hamming weight of any non-zero codeword in $C$.
    \[
        d = \min_{\vec{c} \in C, \vec{c} \neq \vec{0}} wt(\vec{c})
    \]
\end{theorem}
\begin{proof}
    Let $\vec{c}_1, \vec{c}_2 \in C$ be two distinct codewords. Since $C$ is a linear subspace, their difference $\vec{c}_1 - \vec{c}_2$ is also a non-zero codeword in $C$. The distance is $\triangle(\vec{c}_1, \vec{c}_2) = wt(\vec{c}_1 - \vec{c}_2)$. Therefore, the minimum distance between any two distinct codewords is the same as the minimum weight of any non-zero codeword.
\end{proof}

\begin{theorem}
    The minimum distance $d$ of a linear code $C$ with parity-check matrix $H$ is the smallest integer $d$ such that there exist $d$ linearly dependent columns in $H$.
\end{theorem}
\begin{proof}
    A codeword $\vec{c}$ exists if and only if $H\vec{c}^T = \vec{0}$. Let the columns of $H$ be $\vec{h}_1, \dots, \vec{h}_n$. Then $H\vec{c}^T = \sum_{i=1}^n c_i \vec{h}_i$. A non-zero codeword $\vec{c}$ of weight $d$ has exactly $d$ non-zero components, say at indices $i_1, \dots, i_d$. The condition $H\vec{c}^T = \vec{0}$ becomes $\sum_{j=1}^d c_{i_j} \vec{h}_{i_j} = \vec{0}$. This is a linear dependency among the columns $\vec{h}_{i_1}, \dots, \vec{h}_{i_d}$. The minimum distance $d$ corresponds to the smallest set of columns that are linearly dependent.
\end{proof}

\subsection{Dual Codes}

\begin{description}
    \item[Dual Code] For an $[n, k]$ linear code $C$, its \textbf{dual code}, denoted $C^\perp$, is the set of all vectors in $\mathbb{F}_q^n$ that are orthogonal to every codeword in $C$.
    \[
        C^\perp = \{ \vec{v} \in \mathbb{F}_q^n \mid \vec{v} \cdot \vec{c} = 0 \text{ for all } \vec{c} \in C \}
    \]
    If $C$ has generator matrix $G$ and parity-check matrix $H$, then $C^\perp$ is an $[n, n-k]$ code with generator matrix $H$ and parity-check matrix $G$.
\end{description}

\subsection{Families of Codes \& Asymptotics}

    A key goal in coding theory is to find families of codes that have both a high rate ($R=k/n$) and a high relative distance ($\delta=d/n$) as the block length $n$ goes to infinity. We seek families of $[n_i, k_i, d_i]_q$ codes where $n_i \to \infty$ and the ratios $R_i = k_i/n_i$ and $\delta_i = d_i/n_i$ approach desirable limits.

\subsection{Hamming Codes}
    Hamming codes are a famous family of linear codes that are "perfect," meaning they achieve the maximum possible number of codewords for their distance.

\begin{description}
    \item[Hamming Code] For any integer $r \ge 2$, the binary \textbf{Hamming code} $\text{Ham}(r, 2)$ is a code with parameters:
    \begin{itemize}
        \item Block length: $n = 2^r - 1$
        \item Dimension: $k = 2^r - 1 - r$
        \item Minimum distance: $d = 3$
    \end{itemize}
    The parity-check matrix $H$ for this code is an $r \times (2^r-1)$ matrix whose columns consist of all non-zero binary vectors of length $r$. Since any two columns are distinct and non-zero, no two columns are linearly dependent, but many sets of three columns are (e.g., if $\vec{h}_i + \vec{h}_j = \vec{h}_k$), so the distance is exactly 3.

    \item[Decoding Hamming Codes] Hamming codes have a very efficient decoding algorithm. Given a received vector $\vec{y}$, we compute the \textbf{syndrome} $\vec{s} = H\vec{y}^T$.
    \begin{itemize}
        \item If $\vec{s} = \vec{0}$, then $\vec{y}$ is a codeword and we assume no error occurred.
        \item If a single error occurred at position $i$, so that $\vec{y} = \vec{c} + \vec{e}_i$, then the syndrome is $\vec{s} = H(\vec{c}+\vec{e}_i)^T = H\vec{c}^T + H\vec{e}_i^T = H\vec{e}_i^T$. This result is precisely the $i$-th column of $H$.
    \end{itemize}
    By computing the syndrome, we can directly identify the column corresponding to the error position and correct it by flipping the bit.
\end{description}

