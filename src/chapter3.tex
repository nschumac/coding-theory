
\begin{description}
    \item[Union Bound] The probability of one event occurring is no greater than that of the sum of probabilities.

    For any set of events $A_1, A_2, \dots, A_m$,
    \[
        P(A_1 \cup A_2 \cup \dots \cup A_m) \le \sum_{i=1}^m P(A_i)
    \]

    \item[Chernoff Bound] Let $X_1, \dots, X_n$ be independent random variables where $P(X_i=1) = p$ and $P(X_i=0) = 1-p$. Let $X = \sum_{i=1}^n X_i$. The expected value is $E[X] = np$. 
        The Chernoff bound states that for any $\delta > 0$,
    \[
        P(X \ge (1+\delta)np) \le e^{-D(p(1+\delta) || p)n}
    \]
    and
    \[
        P(X \le (1-\delta)np) \le e^{-D(p(1-\delta) || p)n}
    \]
    where $D(a || p) = a \log\frac{a}{p} + (1-a)\log\frac{1-a}{1-p}$ This bound is crucial for proving that random codes are, with high probability, good codes.
\end{description}

\begin{description}
    \item[Intuition] Measure of average uncertainty. A fair coin is unrepdictable and offers a surprising outcome that provides more information than say a loaded coin
    \item[Binary Entropy] For a binary random variable that takes the value 1 with probability $p$ and 0 with probability $1-p$, the \textbf{binary entropy function}, $H_2(p)$, measures its uncertainty in bits.
    \[
        H_2(p) = -p \log_2(p) - (1-p) \log_2(1-p)
    \]
    \begin{itemize}
        \item If $p=0$ or $p=1$, the outcome is certain, and $H_2(p) = 0$.
        \item If $p=0.5$, the outcome is maximally uncertain, and $H_2(0.5) = 1$ bit.
    \end{itemize}
    \item[q-ary Entropy] The concept generalizes to an alphabet of size $q$. If a random variable can take $q$ different values with probabilities $p_1, \dots, p_q$, its entropy is $H_q(p_1, \dots, p_q) = -\sum_{i=1}^q p_i \log_q(p_i)$.
    
    A special case, the probability of a symbol being one psecific value with probability $p$ versus any other $q - 1$ values.
    \[
        H_q(p) = -p\log_q(p) - (1-p)\log_q(1-p) + p\log_q(q-1)
    \]
    useful for estimating hte size of hamming balls
\end{description}

\begin{description}
    \item[Definition] The \textbf{Hamming ball} of radius $r$ around a vector $\vec{c} \in \mathbb{F}_q^n$, denoted $B(\vec{c}, r)$, is the set of all vectors $\vec{y} \in \mathbb{F}_q^n$ such that the Hamming distance $\triangle(\vec{c}, \vec{y}) \le r$.
    
    The \textbf{volume} of this ball, denoted $Vol_q(r, n)$, is the number of vectors it contains. 
    \[
        Vol_q(r, n) = |B(\vec{0}, r)| = \sum_{i=0}^r \binom{n}{i}(q-1)^i
    \]
    The term $\binom{n}{i}$ chooses $i$ positions to have errors, and $(q-1)^i$ accounts for the $q-1$ possible non-zero symbols that can appear in each of those positions.

    \begin{theorem}[Volume of a Hamming Ball]
    For an alphabet of size $q \ge 2$, an error probability $0 \le p \le 1 - 1/q$, and large block length $n$, the volume of a Hamming ball of radius $pn$ is approximately:
    \[
        Vol_q(pn, n) \approx q^{n H_q(p)}
    \]
    where $H_q(p)$ is the q-ary entropy function.
    \end{theorem}
    It states that the number of typical error patterns of a certain density $p$ occupies a "volume" whose size is determined by the entropy. 
\end{description}
